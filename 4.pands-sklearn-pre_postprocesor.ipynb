{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d5a597-2658-4c2e-bdda-944c2dab87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform[prediction]@git+https://github.com/googleapis/python-aiplatform.git@custom-prediction-routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e92f9-980b-43e9-8c76-b2aa02260e96",
   "metadata": {},
   "source": [
    "# Sklearn with Pandas - Custom Prediction Routine to get `.predict_proba()`\n",
    "\n",
    "This is similar to [the other notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_cpr.ipynb) except we will be using pandas and bigquery\n",
    "\n",
    "Topics covered\n",
    "* Training sklearn locally, deploying to endpoint\n",
    "* Saving data as CSV and doing batch predict from GCS\n",
    "* Loading data to BQ, using BQ magics\n",
    "* Running a batch prediction from BQ to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b65e26b-31d7-459e-8f80-92e2c206fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l us-central1 gs://wortz-project-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbe8a06-ed15-40ac-bc2f-d387bb406301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "PROJECT_ID = 'wortz-project-352116' #SET THIS TO YOUR PROJECT ID\n",
    "PREFIX = 'pre-processor-example'\n",
    "BUCKET = f\"gs://wortz-project-bucket/model-artifacts/{PREFIX}\" #BE SURE TO gsutil mb -l <REGION> <LOG_BUCKET> to create the bucket on GCP\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb15f1f6-9bc5-4e82-9c4f-3f9c33882868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data\n",
    "import pandas as pd\n",
    "import numpy as np #for the random integer example\n",
    "\n",
    "# set seed\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "x = np.random.randint(0.0,100.0,size=(10,3))\n",
    "y = np.random.binomial(1, .25, size=(10,1))\n",
    "df = pd.DataFrame(np.append(x, y, axis=1),\n",
    "              index=range(10,20),\n",
    "              columns=['col1','col2','col3','label'],\n",
    "              dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13825936-816a-4b3f-a624-7c7684fc7c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>92.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>73.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>76.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    col1  col2  col3  label\n",
       "10  47.0  83.0  38.0    0.0\n",
       "11  53.0  76.0  24.0    1.0\n",
       "12  15.0  49.0  23.0    0.0\n",
       "13  26.0  30.0  43.0    0.0\n",
       "14  30.0  26.0  58.0    1.0\n",
       "15  92.0  69.0  80.0    0.0\n",
       "16  73.0  47.0  50.0    0.0\n",
       "17  76.0  37.0  34.0    1.0\n",
       "18  38.0  67.0  11.0    0.0\n",
       "19   0.0  75.0  80.0    1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831cdd6-8d18-40aa-88ba-50889c3624a0",
   "metadata": {},
   "source": [
    "# New section - preprocessor creation.\n",
    "\n",
    "In this section we will create a pipeline object that stores a standard scaler \n",
    "using the `PipeLine` class is important as it provides a lot of flexibility and conforms to sklearn's framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0849f09-5f48-4e73-96db-c2fa53c06ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Set the model parameters. \n",
    "n_estimators = 100\n",
    "max_depth = 6\n",
    "max_features = 3\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features)\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('clf', rf)])\n",
    "\n",
    "fitted_pipe = pipe.fit(df[['col1', 'col2', 'col3']], df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9492e9-7037-441a-bcc2-5779e57be327",
   "metadata": {},
   "source": [
    "## At this point you have a single artifact. We implement preprocessing in a couple of ways:\n",
    "1) Store the entire pipeline - may work with prior example (worth testing/considering)\n",
    "2) Break out preprocessing, store artifacts and leverage in preprocessor for cpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d32fb67-b6f8-40fc-b335-95c41c2c3133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_preproc.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "model_artifact_filename = 'model.joblib' #has to be joblib to work with CPR\n",
    "\n",
    "# Save model artifact to local filesystem (doesn't persist)\n",
    "\n",
    "joblib.dump(fitted_pipe.named_steps['clf'], model_artifact_filename)\n",
    "\n",
    "scaler_artifact_filename = 'scaler_preproc.joblib' #has to be joblib to work with CPR\n",
    "\n",
    "# Save model artifact to local filesystem (doesn't persist)\n",
    "\n",
    "joblib.dump(fitted_pipe.named_steps['scale'], scaler_artifact_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07022a43-5b9b-45bc-84af-f43173e627a4",
   "metadata": {},
   "source": [
    "#### Upload the model pipeline to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e83b92-13d2-4881-b27f-091e96af4262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 78.3 KiB/ 78.3 KiB]                                                \n",
      "Operation completed over 1 objects/78.3 KiB.                                     \n",
      "Copying file://scaler_preproc.joblib [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  997.0 B/  997.0 B]                                                \n",
      "Operation completed over 1 objects/997.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp $model_artifact_filename $BUCKET/model/\n",
    "! gsutil cp $scaler_artifact_filename $BUCKET/scaler/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c749d4-18ca-46a0-91f9-432dbf1220e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a generic sklearn container that returns `predict_proba`\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_cpr.ipynb\n",
    "\n",
    "**highly recommend reviewing this notebook first as it breaks down the custom predictor interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf429ed-e23c-47ac-93da-8a70c4e3e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf container_code\n",
    "! mkdir container_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af962a7-3196-4b9a-9c36-3f35fd525b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing container_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile container_code/requirements.txt\n",
    "# fastapi\n",
    "# uvicorn\n",
    "joblib~=1.0\n",
    "numpy>=1.20\n",
    "scikit-learn~=1.0\n",
    "google-cloud-storage>=1.5.0,<2.0.0dev\n",
    "google-cloud-aiplatform[prediction] @ git+https://github.com/googleapis/python-aiplatform.git@custom-prediction-routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d808c149-acef-4d7f-873a-62f372f3393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing container_code/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile container_code/predictor.py\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform.prediction.sklearn.predictor import SklearnPredictor\n",
    "import json\n",
    "\n",
    "class CprPredictor(SklearnPredictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def load(self, gcs_artifacts_uri: str):\n",
    "        \"\"\"Loads the preprocessor and model artifacts.\"\"\"\n",
    "        # super().load(gcs_artifacts_uri)   \n",
    "        gcs_client = storage.Client()\n",
    "        with open(\"model.joblib\", 'wb') as gcs_model:\n",
    "            gcs_client.download_blob_to_file(\n",
    "                gcs_artifacts_uri + \"/model/model.joblib\", gcs_model\n",
    "            )\n",
    "\n",
    "        with open(\"model.joblib\", \"rb\") as f:\n",
    "            self._model = joblib.load(\"model.joblib\")\n",
    "        \n",
    "        #load the scaler\n",
    "        with open(\"scaler_preproc.joblib\", 'wb') as gcs_scaler:\n",
    "            gcs_client.download_blob_to_file(\n",
    "                gcs_artifacts_uri + \"/scaler/scaler_preproc.joblib\", gcs_scaler\n",
    "            )\n",
    "\n",
    "        with open(\"scaler_preproc.joblib\", \"rb\") as f:\n",
    "            scaler_obj = joblib.load(\"scaler_preproc.joblib\") #load the scaler object\n",
    "        \n",
    "        self._preprocessor = scaler_obj #call transform as it's already fitted\n",
    "            \n",
    "    def preprocess(self, prediction_input: dict):\n",
    "        \"\"\"Perform scaling preprocessing\"\"\"\n",
    "        inputs = super().preprocess(prediction_input) #we are using instances format here as we haven't changed the prediction handler (ie data looks the same here as inputs for predict\n",
    "        return self._preprocessor.transform(inputs)\n",
    "\n",
    "    \n",
    "    def predict(self, instances):\n",
    "        outputs = self._model.predict_proba(instances) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a17737-58ad-4819-9b2d-72de6fd6e827",
   "metadata": {},
   "source": [
    "### Build and push container to Artifact Registry\n",
    "#### Build your container\n",
    "To build a custom container, we also need to write an entrypoint of the image that starts the model server. However, with the Custom Prediction Routine feature, you don't need to write the entrypoint anymore. Vertex AI SDK will populate the entrypoint with the custom predictor you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05bf27d9-03c8-46e7-9dd3-45b6586f1316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from container_code.predictor import CprPredictor\n",
    "\n",
    "REPOSITORY = \"custom-preprocess-container-prediction\"  # @param {type:\"string\"}\n",
    "SERVER_IMAGE = \"sklearn-cpr-preprocess-server\"  # @param {type:\"string\"} \n",
    "\n",
    "local_model = LocalModel.create_cpr_model(\n",
    "    \"container_code\",\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\",\n",
    "    predictor=CprPredictor,\n",
    "    requirements_path=\"container_code/requirements.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80330616-933a-4ec9-9322-2dd5815c4947",
   "metadata": {},
   "source": [
    "### Test it out with a locally deployed endpoint\n",
    "Need to generate credentials to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a01981a9-6972-4eb9-b7ba-18689a31b90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"us-central1-docker.pkg.dev/wortz-project-352116/custom-preprocess-container-prediction/sklearn-cpr-preprocess-server\"\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa05b7-4303-4655-9cc0-069be8bcf969",
   "metadata": {},
   "source": [
    "#### Only run once to generate creds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963400a8-e35a-49eb-854f-46a5421901a9",
   "metadata": {},
   "source": [
    "### Create a repository to house your artifacts / images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c73a1426-a67e-4fda-8112-aeceef6d85d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta artifacts repositories create $REPOSITORY \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a36035-b4ad-4ffa-97cd-444ebbbaf03e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a979a4-48a8-4453-90a3-3629957bd878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload the model to Vertex using new Prediction Route Serving Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59eb3650-759a-40a1-bd2f-704daad82410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model.push_image() #push to container registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d8e71d-9702-4c7d-bd15-925278f759d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/679926387543/locations/us-central1/models/8299698706639224832/operations/6891626251677597696\n",
      "Model created. Resource name: projects/679926387543/locations/us-central1/models/8299698706639224832\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/679926387543/locations/us-central1/models/8299698706639224832')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "model = local_model.upload(\n",
    "        display_name='pandas test CLASSIFICATION',\n",
    "        artifact_uri=BUCKET,\n",
    "        description='preprocessor example - pandas test for deploying models to vertex CLASSIFICATION',\n",
    "        labels= {'version': 'v1_1_'}, \n",
    "              \n",
    "        sync=True, #false will not bind up your notebook instance with the creation operation\n",
    "    ) \n",
    "# model = aiplatform.Model('projects/679926387543/locations/us-central1/models/5966834099661307904')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bee664-c1bd-438a-b8e3-6bb2f09129a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/679926387543/locations/us-central1/endpoints/7051678242322776064/operations/1548668243755925504\n",
      "Endpoint created. Resource name: projects/679926387543/locations/us-central1/endpoints/7051678242322776064\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/679926387543/locations/us-central1/endpoints/7051678242322776064')\n",
      "Deploying model to Endpoint : projects/679926387543/locations/us-central1/endpoints/7051678242322776064\n",
      "Deploy Endpoint model backing LRO: projects/679926387543/locations/us-central1/endpoints/7051678242322776064/operations/5585582359740153856\n",
      "Endpoint model deployed. Resource name: projects/679926387543/locations/us-central1/endpoints/7051678242322776064\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")\n",
    "# endpoint = aiplatform.Endpoint('projects/679926387543/locations/us-central1/endpoints/8555880517864521728')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca04949-78d9-4885-8b11-069bb12f9e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.79, 0.21], [0.24, 0.76]], deployed_model_id='2882294965424095232', explanations=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(instances=[[47.7, 83.1, 38.7], [53.6, 76.1, 24.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecbb6a-059e-4dee-bcb7-9974a518965c",
   "metadata": {},
   "source": [
    "# You should be able to see the logging ops by searching for `aiplatform.googleapis.com`\n",
    "+ Make sure you click `show query` slider in case there are other limitations\n",
    "![](images/log_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8328221-1c7c-4c65-bf71-59b59b75a7b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25239/4292456183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minstances_formatted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m predict_response = model.predict(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mrequest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances_formatted_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame(np.random.randint(0.0,100.0,size=(10,3)), # we will do batch predictions based on this\n",
    "              index=range(10,20),\n",
    "              columns=['col1','col2','col3'],\n",
    "              dtype='float64')\n",
    "\n",
    "instances_formatted_data = df2.to_numpy().tolist()\n",
    "\n",
    "predict_response = model.predict(\n",
    "        request_file=instances_formatted_data,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98f67d-b47e-43de-a41e-0e927bff6e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Expected output\n",
    "From documentation:\n",
    "```\n",
    "array([[0.8 , 0.2 ],\n",
    "       [0.38, 0.62],\n",
    "       [0.61, 0.39],\n",
    "       [0.65, 0.35],\n",
    "       [0.56, 0.44],\n",
    "       [0.63, 0.37],\n",
    "       [0.55, 0.45],\n",
    "       [0.43, 0.57],\n",
    "       [0.43, 0.57],\n",
    "       [0.38, 0.62]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994f47d-37e8-4992-9c38-762a713818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import csv\n",
    "\n",
    "# save the csv with the header, no index\n",
    "df2.to_csv('df2.csv', index=False)\n",
    "\n",
    "data_directory = BUCKET + \"/data\"\n",
    "storage_path = os.path.join(data_directory, 'df2.csv')\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "blob.upload_from_filename(\"df2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a10c6c-18c1-4bf8-9641-61e4e1fe4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "        job_display_name='pandas batch predict job sklearn - VALUES JSON',\n",
    "        gcs_source=storage_path,\n",
    "        gcs_destination_prefix=BUCKET+\"/predictions\",\n",
    "        machine_type='n1-standard-2',\n",
    "        instances_format='csv', #This is key to parsing CSV input\n",
    "        # accelerator_count=accelerator_count,\n",
    "        # accelerator_type=accelerator_type, #if you want gpus\n",
    "        starting_replica_count=1,\n",
    "        max_replica_count=2,\n",
    "        sync=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903dc3b1-3458-447d-ab53-a25774d6c2d6",
   "metadata": {},
   "source": [
    "### When successful you should see this\n",
    "```\n",
    "{\"instance\": [16.0, 64.0, 61.0], \"prediction\": [0.63, 0.37]}\n",
    "{\"instance\": [83.0, 27.0, 87.0], \"prediction\": [0.35, 0.65]}\n",
    "{\"instance\": [96.0, 83.0, 57.0], \"prediction\": [0.68, 0.32]}\n",
    "{\"instance\": [11.0, 62.0, 17.0], \"prediction\": [0.89, 0.11]}\n",
    "{\"instance\": [61.0, 28.0, 1.0], \"prediction\": [0.36, 0.64]}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
